model,is_moe,num_hidden_layers,fraction_dense_layers,hidden_size,query_to_residual_stream,kv_stream_to_residual_stream,kv_flexibility,expert_routing_bits,dense_ffn_to_residual_stream,expert_size_to_residual_stream,moe_active_ffn_to_residual_stream,moe_total_ffn_to_residual_stream,fraction_experts_shared,shared_experts,all_experts_total_size,expert_sparsity,active_experts_total_size,dense_ffn_intermediate_size,routed_expert_size,experts_per_token,attention_num_heads,attention_general_type,attention_num_kv_heads,dense_layers,moe_layers,attention_head_dim,kv_cache_dim_kv_per_token_layer,kv_cache_dim_ratio_vs_mha,kv_rank_proxy_queryspace,kv_capacity_proxy,attention_notes,kv_strain_notes,attention_specific_type,max_position_embeddings,rope_theta,q_dim_per_token_layer,total_experts,routed_experts,mlp_notes,source_keys
MiniMaxAI_MiniMax-Text-01,1,80,0,6144,1.33333333333333,0.333333333333333,3.2,9.0,,1.5,3,48,0.0,0.0,294912.0,0.0625,18432.0,,9216.0,2.0,64,GQA,8,0,80.0,128,2048.0,0.125,8.0,6492.0,,,"GQA [h=64, kv=8, hd=128]",10240000,10000000.0,8192.0,32.0,32.0,moe layer indices unknown; assumed all layers are MoE; MoE expert width not explicit; used intermediate_size as routed expert width (may be ambiguous),"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.num_local_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_shared_width_key"": ""primary.shared_intermediate_size"", ""moe_routed_width_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""router_noise_key"": ""router_jitter_noise"", ""aux_loss_key"": ""router_aux_loss_coef""}"
PrimeIntellect_INTELLECT-3-FP8,1,46,0.0217391304347826,4096,3,0.5,3.2,40.4,2.671875,0.34375,3.09375,44.34375,0.11,1.0,181632.0,0.0625,12672.0,10944.0,1408.0,9.0,96,GQA,8,1,45.0,128,2048.0,0.0833333333333333,8.0,6492.0,,,"GQA [h=96, kv=8, hd=128]",131072,1000000.0,12288.0,129.0,128.0,dense prefix from first_k_dense_replace; moe layers inferred as all after dense prefix; shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.n_routed_experts"", ""shared_experts_key"": ""primary.n_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""partial_rope_key"": ""partial_rotary_factor"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""qk_norm_key"": ""use_qk_norm""}"
Qwen_Qwen3-235B-A22B-Instruct-2507,1,94,0,4096,2,0.25,2.3,40.4,,0.375,3,48,0.00,0.0,196608.0,0.0625,12288.0,,1536.0,8.0,64,GQA,4,0,94.0,128,1024.0,0.0625,4.0,2377.7,,,"GQA [h=64, kv=4, hd=128]",262144,5000000.0,8192.0,128.0,128.0,dense layers from mlp_only_layers,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.num_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""aux_loss_key"": ""router_aux_loss_coef""}"
Qwen_Qwen3-Coder-480B-A35B-Instruct,1,62,0,6144,2,0.333333333333333,3.2,43.0,,0.416666666666667,3.33333333333333,66.6666666666667,0.00,0.0,409600.0,0.05,20480.0,,2560.0,8.0,96,GQA,8,0,62.0,128,2048.0,0.0833333333333333,8.0,6492.0,,,"GQA [h=96, kv=8, hd=128]",262144,10000000.0,12288.0,160.0,160.0,dense layers from mlp_only_layers,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.num_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""moe_shared_width_key"": ""primary.shared_expert_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""qk_norm_key"": ""use_qk_norm"", ""aux_loss_key"": ""router_aux_loss_coef""}"
ai21labs_AI21-Jamba-Large-1.7,1,72,0.5,8192,1,0.25,3.2,6.9,3,3,6,48,0.00,0.0,393216.0,0.125,49152.0,24576.0,24576.0,2.0,64,GQA,8,36,36.0,128,2048.0,0.125,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=64, kv=8, hd=128]",262144,,8192.0,16.0,16.0,moe layers from expert_layer_period/offset; Claude (gated sources) confirms intermediate_size=24576 applies to dense FFN and per-expert FFN; num_experts_per_tok=2 of num_experts=16; no shared experts,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.num_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""aux_loss_key"": ""router_aux_loss_coef""}"
baidu_ERNIE-4.5-300B-A47B-PT,1,54,0.0555555555555556,8192,1,0.25,3.2,32.0,3.5,0.4375,3.5,28,0.00,0.0,229376.0,0.125,28672.0,28672.0,3584.0,8.0,64,GQA,8,3,51.0,128,2048.0,0.125,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=64, kv=8, hd=128]",131072,500000.0,8192.0,64.0,64.0,moe layers from moe_layer_start/end/interval,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.moe_num_experts"", ""shared_experts_key"": ""primary.moe_num_shared_experts"", ""active_experts_key"": ""primary.moe_k"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""aux_loss_key"": ""router_aux_loss_coef""}"
baidu_ernie-4.5-vl-424b-a47b,1,54,0,8192,1,0.25,3.2,32.0,,0.3125,2.5,20,0.00,0.0,163840.0,0.125,20480.0,,2560.0,8.0,64,GQA,8,0,54.0,128,2048.0,0.125,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=64, kv=8, hd=128]",131072,500000.0,8192.0,64.0,64.0,moe layers from moe_layer_start/end/interval; routed expert width list primary.moe_intermediate_size length 2 not alignable; used simple mean,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.moe_num_experts"", ""active_experts_key"": ""primary.moe_k"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
deepcogito_cogito-671b-v2.1,1,61,0.0491803278688525,7168,1.14285714285714,0.142857142857143,9.0,48.5,2.57142857142857,0.285714285714286,2.57142857142857,73.4285714285714,0.11,1.0,526336.0,0.03125,18432.0,18432.0,2048.0,9.0,128,MLA,128,3,58.0,64,1024.0,0.0625,512.0,9218.9,,"MLA: kv_size_proxy uses kv_lora_rank (latent KV cache); MLA dims: kv_lora_rank=512, qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128","MLA (qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128, kv_lora_rank=512, q_lora_rank=1536) [h=128, kv=128, hd=64]",163840,10000.0,8192.0,257.0,256.0,dense prefix from first_k_dense_replace; moe layers inferred from moe_layer_freq (+dense prefix if any); shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.n_routed_experts"", ""shared_experts_key"": ""primary.n_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
deepcogito_cogito-v2-preview-llama-109b-moe,1,48,0,5120,1,0.4,3.2,4.0,,1.6,1.6,25.6,0.00,0.0,131072.0,0.0625,8192.0,,8192.0,1.0,40,GQA,8,0,48.0,128,2048.0,0.2,8.0,6492.0,,,"GQA [h=40, kv=8, hd=128]",262144,500000.0,5120.0,16.0,16.0,interleave_moe_layer_step=1; assumed all layers MoE; MoE expert width not explicit; used intermediate_size as routed expert width (may be ambiguous),"{""primary_config"": ""text_config"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.num_local_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""qk_norm_key"": ""use_qk_norm"", ""attn_scale_key"": ""attn_scale"", ""router_noise_key"": ""router_jitter_noise"", ""aux_loss_key"": ""router_aux_loss_coef""}"
deepseek-ai_DeepSeek-Prover-V2-671B,1,61,0.0491803278688525,7168,1,0.142857142857143,9.0,48.5,2.57142857142857,0.285714285714286,2.57142857142857,73.4285714285714,0.11,1.0,526336.0,0.03125,18432.0,18432.0,2048.0,9.0,128,MLA,128,3,58.0,56,1024.0,0.0714285714285714,512.0,9218.9,derived head_dim from hidden_size/num_attention_heads,"MLA: kv_size_proxy uses kv_lora_rank (latent KV cache); MLA dims: kv_lora_rank=512, qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128","MLA (qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128, kv_lora_rank=512, q_lora_rank=1536) [h=128, kv=128, hd=56]",163840,10000.0,7168.0,257.0,256.0,dense prefix from first_k_dense_replace; moe layers inferred from moe_layer_freq (+dense prefix if any); shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.n_routed_experts"", ""shared_experts_key"": ""primary.n_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
deepseek-ai_DeepSeek-V3-0324,1,61,0.0491803278688525,7168,1,0.142857142857143,9.0,48.5,2.57142857142857,0.285714285714286,2.57142857142857,73.4285714285714,0.11,1.0,526336.0,0.03125,18432.0,18432.0,2048.0,9.0,128,MLA,128,3,58.0,56,1024.0,0.0714285714285714,512.0,9218.9,derived head_dim from hidden_size/num_attention_heads,"MLA: kv_size_proxy uses kv_lora_rank (latent KV cache); MLA dims: kv_lora_rank=512, qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128","MLA (qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128, kv_lora_rank=512, q_lora_rank=1536) [h=128, kv=128, hd=56]",163840,10000.0,7168.0,257.0,256.0,dense prefix from first_k_dense_replace; moe layers inferred from moe_layer_freq (+dense prefix if any); shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.n_routed_experts"", ""shared_experts_key"": ""primary.n_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
deepseek-ai_DeepSeek-V3.1,1,61,0.0491803278688525,7168,1,0.142857142857143,9.0,48.5,2.57142857142857,0.285714285714286,2.57142857142857,73.4285714285714,0.11,1.0,526336.0,0.03125,18432.0,18432.0,2048.0,9.0,128,MLA,128,3,58.0,56,1024.0,0.0714285714285714,512.0,9218.9,derived head_dim from hidden_size/num_attention_heads,"MLA: kv_size_proxy uses kv_lora_rank (latent KV cache); MLA dims: kv_lora_rank=512, qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128","MLA (qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128, kv_lora_rank=512, q_lora_rank=1536) [h=128, kv=128, hd=56]",163840,10000.0,7168.0,257.0,256.0,dense prefix from first_k_dense_replace; moe layers inferred from moe_layer_freq (+dense prefix if any); shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.n_routed_experts"", ""shared_experts_key"": ""primary.n_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
deepseek_deepseek-r1,1,61,0.0491803278688525,7168,1,0.142857142857143,9.0,48.5,2.57142857142857,0.285714285714286,2.57142857142857,73.4285714285714,0.11,1.0,526336.0,0.03125,18432.0,18432.0,2048.0,9.0,128,MLA,128,3,58.0,56,1024.0,0.0714285714285714,512.0,9218.9,derived head_dim from hidden_size/num_attention_heads,"MLA: kv_size_proxy uses kv_lora_rank (latent KV cache); MLA dims: kv_lora_rank=512, qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128","MLA (qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128, kv_lora_rank=512, q_lora_rank=1536) [h=128, kv=128, hd=56]",163840,10000.0,7168.0,257.0,256.0,dense prefix from first_k_dense_replace; moe layers inferred from moe_layer_freq (+dense prefix if any); shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.n_routed_experts"", ""shared_experts_key"": ""primary.n_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
deepseek_deepseek-r1-0528,1,61,0.0491803278688525,7168,1,0.142857142857143,9.0,48.5,2.57142857142857,0.285714285714286,2.57142857142857,73.4285714285714,0.11,1.0,526336.0,0.03125,18432.0,18432.0,2048.0,9.0,128,MLA,128,3,58.0,56,1024.0,0.0714285714285714,512.0,9218.9,derived head_dim from hidden_size/num_attention_heads,"MLA: kv_size_proxy uses kv_lora_rank (latent KV cache); MLA dims: kv_lora_rank=512, qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128","MLA (qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128, kv_lora_rank=512, q_lora_rank=1536) [h=128, kv=128, hd=56]",163840,10000.0,7168.0,257.0,256.0,dense prefix from first_k_dense_replace; moe layers inferred from moe_layer_freq (+dense prefix if any); shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.n_routed_experts"", ""shared_experts_key"": ""primary.n_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
deepseek_deepseek-v3.1-terminus,1,61,0.0491803278688525,7168,1,0.142857142857143,9.0,48.5,2.57142857142857,0.285714285714286,2.57142857142857,73.4285714285714,0.11,1.0,526336.0,0.03125,18432.0,18432.0,2048.0,9.0,128,MLA,128,3,58.0,56,1024.0,0.0714285714285714,512.0,9218.9,derived head_dim from hidden_size/num_attention_heads,"MLA: kv_size_proxy uses kv_lora_rank (latent KV cache); MLA dims: kv_lora_rank=512, qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128","MLA (qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128, kv_lora_rank=512, q_lora_rank=1536) [h=128, kv=128, hd=56]",163840,10000.0,7168.0,257.0,256.0,dense prefix from first_k_dense_replace; moe layers inferred from moe_layer_freq (+dense prefix if any); shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.n_routed_experts"", ""shared_experts_key"": ""primary.n_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
deepseek_deepseek-v3.1-terminus_exacto,1,61,0.0491803278688525,7168,1,0.142857142857143,9.0,48.5,2.57142857142857,0.285714285714286,2.57142857142857,73.4285714285714,0.11,1.0,526336.0,0.03125,18432.0,18432.0,2048.0,9.0,128,MLA,128,3,58.0,56,1024.0,0.0714285714285714,512.0,9218.9,derived head_dim from hidden_size/num_attention_heads,"MLA: kv_size_proxy uses kv_lora_rank (latent KV cache); MLA dims: kv_lora_rank=512, qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128","MLA (qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128, kv_lora_rank=512, q_lora_rank=1536) [h=128, kv=128, hd=56]",163840,10000.0,7168.0,257.0,256.0,dense prefix from first_k_dense_replace; moe layers inferred from moe_layer_freq (+dense prefix if any); shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.n_routed_experts"", ""shared_experts_key"": ""primary.n_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
deepseek_deepseek-v3.2,1,61,0.0491803278688525,7168,1,0.142857142857143,9.0,48.5,2.57142857142857,0.285714285714286,2.57142857142857,73.4285714285714,0.11,1.0,526336.0,0.03125,18432.0,18432.0,2048.0,9.0,128,MLA,128,3,58.0,56,1024.0,0.0714285714285714,512.0,9218.9,derived head_dim from hidden_size/num_attention_heads,"MLA: kv_size_proxy uses kv_lora_rank (latent KV cache); MLA dims: kv_lora_rank=512, qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128","MLA (qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128, kv_lora_rank=512, q_lora_rank=1536) [h=128, kv=128, hd=56]",163840,10000.0,7168.0,257.0,256.0,dense prefix from first_k_dense_replace; moe layers inferred from moe_layer_freq (+dense prefix if any); shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.n_routed_experts"", ""shared_experts_key"": ""primary.n_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
deepseek_deepseek-v3.2-exp,1,61,0.0491803278688525,7168,1,0.142857142857143,9.0,48.5,2.57142857142857,0.285714285714286,2.57142857142857,73.4285714285714,0.11,1.0,526336.0,0.03125,18432.0,18432.0,2048.0,9.0,128,MLA,128,3,58.0,56,1024.0,0.0714285714285714,512.0,9218.9,derived head_dim from hidden_size/num_attention_heads,"MLA: kv_size_proxy uses kv_lora_rank (latent KV cache); MLA dims: kv_lora_rank=512, qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128","MLA (qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128, kv_lora_rank=512, q_lora_rank=1536) [h=128, kv=128, hd=56]",163840,10000.0,7168.0,257.0,256.0,dense prefix from first_k_dense_replace; moe layers inferred from moe_layer_freq (+dense prefix if any); shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.n_routed_experts"", ""shared_experts_key"": ""primary.n_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
deepseek_deepseek-v3.2-speciale,1,61,0.0491803278688525,7168,1,0.142857142857143,9.0,48.5,2.57142857142857,0.285714285714286,2.57142857142857,73.4285714285714,0.11,1.0,526336.0,0.03125,18432.0,18432.0,2048.0,9.0,128,MLA,128,3,58.0,56,1024.0,0.0714285714285714,512.0,9218.9,derived head_dim from hidden_size/num_attention_heads,"MLA: kv_size_proxy uses kv_lora_rank (latent KV cache); MLA dims: kv_lora_rank=512, qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128","MLA (qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128, kv_lora_rank=512, q_lora_rank=1536) [h=128, kv=128, hd=56]",163840,10000.0,7168.0,257.0,256.0,dense prefix from first_k_dense_replace; moe layers inferred from moe_layer_freq (+dense prefix if any); shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.n_routed_experts"", ""shared_experts_key"": ""primary.n_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
inclusionai_ling-1t,1,80,0.05,8192,1,0.25,3.2,48.5,2.25,0.25,2.25,64.25,0.11,1.0,526336.0,0.03125,18432.0,18432.0,2048.0,9.0,64,GQA,8,4,76.0,128,2048.0,0.125,8.0,6492.0,,,"GQA [h=64, kv=8, hd=128]",32768,600000.0,8192.0,257.0,256.0,dense prefix from first_k_dense_replace; moe layers inferred as all after dense prefix; shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.num_experts"", ""shared_experts_key"": ""primary.num_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""attention_dropout_key"": ""attention_dropout"", ""emb_dropout_key"": ""embedding_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""qk_norm_key"": ""use_qk_norm"", ""router_score_key"": ""score_function""}"
inclusionai_ling-flash-2.0,1,32,0.03125,4096,1,0.25,2.3,48.5,2.25,0.25,2.25,64.25,0.11,1.0,263168.0,0.03125,9216.0,9216.0,1024.0,9.0,32,GQA,4,1,31.0,128,1024.0,0.125,4.0,2377.7,,,"GQA [h=32, kv=4, hd=128]",32768,600000.0,4096.0,257.0,256.0,dense prefix from first_k_dense_replace; moe layers inferred as all after dense prefix; shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.num_experts"", ""shared_experts_key"": ""primary.num_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""partial_rope_key"": ""partial_rotary_factor"", ""attention_dropout_key"": ""attention_dropout"", ""emb_dropout_key"": ""embedding_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""qk_norm_key"": ""use_qk_norm"", ""router_score_key"": ""score_function""}"
inclusionai_ming-flash-omni-preview,1,32,0.03125,4096,1,0.25,2.3,48.5,2.25,0.25,2.25,64.25,0.11,1.0,263168.0,0.03125,9216.0,9216.0,1024.0,9.0,32,GQA,4,1,31.0,128,1024.0,0.125,4.0,2377.7,,,"GQA [h=32, kv=4, hd=128, sliding_window=4096]",32768,600000.0,4096.0,257.0,256.0,dense prefix from first_k_dense_replace; moe layers inferred as all after dense prefix; shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""llm_config"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.num_experts"", ""shared_experts_key"": ""primary.num_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""attn_sliding_window_key"": ""sliding_window"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""partial_rope_key"": ""partial_rotary_factor"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""emb_dropout_key"": ""embedding_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""router_temp_key"": ""temperature"", ""router_score_key"": ""router_type""}"
inclusionai_ring-1t,1,80,0.05,8192,1,0.25,3.2,48.5,2.25,0.25,2.25,64.25,0.11,1.0,526336.0,0.03125,18432.0,18432.0,2048.0,9.0,64,GQA,8,4,76.0,128,2048.0,0.125,8.0,6492.0,,,"GQA [h=64, kv=8, hd=128]",65536,600000.0,8192.0,257.0,256.0,dense prefix from first_k_dense_replace; moe layers inferred as all after dense prefix; shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.num_experts"", ""shared_experts_key"": ""primary.num_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""attention_dropout_key"": ""attention_dropout"", ""emb_dropout_key"": ""embedding_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""qk_norm_key"": ""use_qk_norm"", ""router_score_key"": ""score_function""}"
inclusionai_ring-flash-2.0,1,32,0.03125,4096,1,0.25,2.3,48.5,2.25,0.25,2.25,64.25,0.11,1.0,263168.0,0.03125,9216.0,9216.0,1024.0,9.0,32,GQA,4,1,31.0,128,1024.0,0.125,4.0,2377.7,,,"GQA [h=32, kv=4, hd=128]",32768,600000.0,4096.0,257.0,256.0,dense prefix from first_k_dense_replace; moe layers inferred as all after dense prefix; shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.num_experts"", ""shared_experts_key"": ""primary.num_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""partial_rope_key"": ""partial_rotary_factor"", ""attention_dropout_key"": ""attention_dropout"", ""emb_dropout_key"": ""embedding_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""qk_norm_key"": ""use_qk_norm"", ""router_score_key"": ""score_function""}"
meituan-longcat_LongCat-Flash-Chat,1,28,0,6144,1,0.166666666666667,9.0,79.0,,0.333333333333333,4,170.666666666667,0.00,0.0,1048576.0,0.0234375,24576.0,,2048.0,12.0,64,MLA,64,0,28.0,96,1024.0,0.0833333333333333,512.0,9218.9,num_key_value_heads missing; assumed equal to num_attention_heads; derived head_dim from hidden_size/num_attention_heads,"MLA: kv_size_proxy uses kv_lora_rank (latent KV cache); MLA dims: kv_lora_rank=512, qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128","MLA (qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128, kv_lora_rank=512, q_lora_rank=1536) [h=64, kv=64, hd=96]",131072,10000000.0,6144.0,512.0,512.0,"moe layer indices unknown; assumed all layers are MoE; Perplexity: According to the LongCat-Flash technical report, each Transformer layer contains 512 FFN experts and 256 zero-computation experts, and the router always selects exactly K = 12 experts per token from this combined pool, so the effective MoE top-k for routed (computing) experts is fixed at 12 across all MoE layers rather than varying by layer or a per-layer computation schedule.[2] The report further states that the dense-path FFNs use an intermediate size of 12288 while each MoE FFN expert uses an intermediate size of 2048, and it does not describe any separate shared experts, implying a pure routed-expert design without distinct shared-expert MLP blocks.[2][5] The dynamic-computation behavior the authors emphasize comes from mixing standard FFN experts with zero-computation experts and a PID-controlled expert-bias mechanism that changes how many of the 12 selected experts are real FFNs versus zero-computation ones per token, not from varying the per-layer moe_topk.[2][5][7]; Perplexity note: model includes zero-computation experts; active_total assumes top-k are computing experts (upper bound).","{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.ffn_hidden_size"", ""routed_experts_key"": ""primary.n_routed_experts"", ""active_experts_key"": ""primary.moe_topk"", ""moe_routed_width_key"": ""primary.expert_ffn_hidden_size"", ""external_perplexity_override"": true, ""attn_heads_key"": ""num_attention_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""attention_dropout_key"": ""attention_dropout""}"
meta-llama_Llama-4-Maverick-17B-128E-Instruct,1,64,0.671875,5120,1,0.4,3.2,40.4,,1.6,12.8,204.8,0.00,0.0,1048576.0,0.0625,65536.0,,8192.0,8.0,40,GQA,8,43,21.0,128,2048.0,0.2,8.0,6492.0,,,"GQA [h=40, kv=8, hd=128]",1048576,500000.0,5120.0,128.0,128.0,"moe layers inferred from interleave_moe_layer_step; MoE expert width not explicit; used intermediate_size as routed expert width (may be ambiguous); Perplexity: Public tooling that reads the Maverick GGUF/weight metadata (e.g., llama.cpp logs) shows llama4.interleave_moe_layer_step = 2 and llama4.expert_feed_forward_length = 8192 for Llama-4-Maverick-17B-128E, and reports that Maverick uses interleaving MoE layers for every odd layer, in a Dense->MoE->Dense pattern.[8] With num_hidden_layers = 64, this means 21 MoE layers (at layer indices 1, 3, 5, …, 41) and 43 purely dense layers (indices 0, 2, 4, …, 62 plus the final layer 63), which is consistent with an interleaving step of 2 starting from the first MoE block at layer 1 rather than layer 0.[8] The same GGUF metadata and downstream configs (TensorRT-LLM, vLLM) interpret expert_feed_forward_length as the per‑expert FFN hidden dimension and map num_local_experts and num_experts_per_tok from the HF-style config to the MoE configuration; for Maverick these are 128 experts total and top_k = num_experts_per_tok = 8 routed experts per token, with no shared experts indicated.[3][6][8][9][20] Meta’s and partner model cards state that Llama 4 Maverick is a 17B-active-parameter, 128‑expert MoE model but do not expose an additional shared-expert pool or a distinct dense MLP width, so the dense-only layers’ intermediate size is not published in those sources and is left null here.[5][7][11][19][20]; Perplexity-derived Maverick layer counts/top-k differ from HF-style config.json in ZIP (which reports 48 layers and num_experts_per_tok=1).","{""primary_config"": ""text_config"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size_mlp"", ""routed_experts_key"": ""primary.num_local_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.intermediate_size"", ""external_perplexity_override"": true, ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""qk_norm_key"": ""use_qk_norm"", ""router_noise_key"": ""router_jitter_noise"", ""aux_loss_key"": ""router_aux_loss_coef""}"
meta-llama_Llama-4-Scout-17B-16E,1,48,0.666666666666667,5120,1,0.4,3.2,4.0,3.2,1.6,1.6,25.6,0.00,0.0,131072.0,0.0625,8192.0,16384.0,8192.0,1.0,40,GQA,8,32,16.0,128,2048.0,0.2,8.0,6492.0,,,"GQA [h=40, kv=8, hd=128]",262144,500000.0,5120.0,16.0,16.0,"interleave_moe_layer_step=1; assumed all layers MoE; MoE expert width not explicit; used intermediate_size as routed expert width (may be ambiguous); Perplexity: According to the Transformers Llama4TextConfig documentation for the 109B/Scout variant, num_hidden_layers=48, intermediate_size=8192, intermediate_size_mlp=16384, num_local_experts=16, and num_experts_per_tok=1. The HF config comments state that this configuration ""will yield a similar configuration to that of the Llama4 109B, e.g. meta-llama/Llama-4-Scout-17B-16E"". This matches Meta’s public description of Llama 4 Scout as a 17B-active / 109B-total MoE model with 16 experts and top‑1 routing (one expert per token). In this design, dense layers use intermediate_size_mlp as the FFN hidden dimension (16384), while MoE layers use intermediate_size per expert (8192) so that 1 active expert per token yields ~17B activated parameters overall when combined with attention and other weights. The interleave_moe_layer_step field is set to 1 for the Scout/109B config. In the reference implementation used by Hugging Face and Meta, this step value is interpreted as “every Nth transformer block is a MoE block starting from the first block index that is marked as MoE,” and the explicit moe_layers mask is used to mark exactly half of the 48 layers as MoE; for Scout this results in 16 MoE blocks and 32 dense blocks, not 48 MoE blocks. Meta’s model card and blog confirm 16 experts with no mention of shared experts, and the HF config does not expose separate shared‑expert dimensions or counts, implying that only routed experts are present and shared_experts=0. Because the public configs do not list a per-layer MoE mask for Scout, the precise layer indices that are MoE vs dense (e.g., odd vs even) are not fully reconstructible from the high-level docs alone; only the aggregate counts and expert dimensions can be stated with confidence.; Perplexity-derived Scout dense/moe layer counts may rely on implementation details not present in the ZIP config; treat dense/moe layer split as best-available estimate.","{""primary_config"": ""text_config"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.num_local_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.intermediate_size"", ""external_perplexity_override"": true, ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""qk_norm_key"": ""use_qk_norm"", ""router_noise_key"": ""router_jitter_noise"", ""aux_loss_key"": ""router_aux_loss_coef""}"
minimax_minimax-m1,1,80,0,6144,1.33333333333333,0.333333333333333,3.2,9.0,,1.5,3,48,0.00,0.0,294912.0,0.0625,18432.0,,9216.0,2.0,64,GQA,8,0,80.0,128,2048.0,0.125,8.0,6492.0,,,"GQA [h=64, kv=8, hd=128]",10240000,10000000.0,8192.0,32.0,32.0,moe layer indices unknown; assumed all layers are MoE; MoE expert width not explicit; used intermediate_size as routed expert width (may be ambiguous),"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.num_local_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_shared_width_key"": ""primary.shared_intermediate_size"", ""moe_routed_width_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""router_noise_key"": ""router_jitter_noise"", ""aux_loss_key"": ""router_aux_loss_coef""}"
minimax_minimax-m2,1,62,0,3072,2,0.666666666666667,3.2,48.5,,0.5,4,128,0.00,0.0,393216.0,0.03125,12288.0,,1536.0,8.0,48,GQA,8,0,62.0,128,2048.0,0.1666666666666670,8.0,6492.0,,,"GQA [h=48, kv=8, hd=128]",196608,5000000.0,6144.0,256.0,256.0,moe layer indices unknown; assumed all layers are MoE; MoE expert width not explicit; used intermediate_size as routed expert width (may be ambiguous),"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.num_local_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_shared_width_key"": ""primary.shared_intermediate_size"", ""moe_routed_width_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""qk_norm_key"": ""use_qk_norm"", ""router_noise_key"": ""router_jitter_noise"", ""aux_loss_key"": ""router_aux_loss_coef""}"
mistralai_Mistral-Large-3-675B-Instruct-2512,1,61,0,7168,3.42857142857143,0.142857142857143,9.0,23.3,,0.571428571428571,2.85714285714286,73.7142857142857,0.20,1.0,528384.0,0.03125,20480.0,,4096.0,5.0,128,MLA,128,0,61.0,192,1024.0,0.0208333333333333,512.0,9218.9,,"MLA: kv_size_proxy uses kv_lora_rank (latent KV cache); MLA dims: kv_lora_rank=512, qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128","MLA (qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128, kv_lora_rank=512, q_lora_rank=1536) [h=128, kv=128, hd=192]",294912,10000.0,24576.0,129.0,128.0,moe layer indices unknown; assumed all layers are MoE; shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.n_layers"", ""hidden_key"": ""primary.dim"", ""dense_ffn_key"": ""primary.hidden_dim"", ""routed_experts_key"": ""moe.num_experts"", ""shared_experts_key"": ""moe.num_shared_experts"", ""active_experts_key"": ""moe.num_experts_per_tok"", ""moe_routed_width_key"": ""moe.expert_hidden_dim"", ""attn_heads_key"": ""n_heads"", ""attn_kv_heads_key"": ""n_kv_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta""}"
moonshotai_Kimi-K2-Instruct,1,61,0.0163934426229508,7168,1,0.142857142857143,9.0,53.3,2.57142857142857,0.285714285714286,2.57142857142857,110,0.11,1.0,788480.0,0.0208333333333333,18432.0,18432.0,2048.0,9.0,64,MLA,64,1,60.0,112,1024.0,0.0714285714285714,512.0,9218.9,derived head_dim from hidden_size/num_attention_heads,"MLA: kv_size_proxy uses kv_lora_rank (latent KV cache); MLA dims: kv_lora_rank=512, qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128","MLA (qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128, kv_lora_rank=512, q_lora_rank=1536) [h=64, kv=64, hd=112]",131072,50000.0,7168.0,385.0,384.0,dense prefix from first_k_dense_replace; moe layers inferred from moe_layer_freq (+dense prefix if any); shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.n_routed_experts"", ""shared_experts_key"": ""primary.n_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
moonshotai_Kimi-K2-Instruct-0905,1,61,0.0163934426229508,7168,1,0.142857142857143,9.0,53.3,2.57142857142857,0.285714285714286,2.57142857142857,110,0.11,1.0,788480.0,0.0208333333333333,18432.0,18432.0,2048.0,9.0,64,MLA,64,1,60.0,112,1024.0,0.0714285714285714,512.0,9218.9,derived head_dim from hidden_size/num_attention_heads,"MLA: kv_size_proxy uses kv_lora_rank (latent KV cache); MLA dims: kv_lora_rank=512, qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128","MLA (qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128, kv_lora_rank=512, q_lora_rank=1536) [h=64, kv=64, hd=112]",262144,50000.0,7168.0,385.0,384.0,dense prefix from first_k_dense_replace; moe layers inferred from moe_layer_freq (+dense prefix if any); shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.n_routed_experts"", ""shared_experts_key"": ""primary.n_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""router_temp_key"": ""temperature""}"
moonshotai_Kimi-K2-Thinking,1,61,0.0163934426229508,7168,1,0.142857142857143,9.0,53.3,2.57142857142857,0.285714285714286,2.57142857142857,110,0.11,1.0,788480.0,0.0208333333333333,18432.0,18432.0,2048.0,9.0,64,MLA,64,1,60.0,112,1024.0,0.0714285714285714,512.0,9218.9,derived head_dim from hidden_size/num_attention_heads,"MLA: kv_size_proxy uses kv_lora_rank (latent KV cache); MLA dims: kv_lora_rank=512, qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128","MLA (qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128, kv_lora_rank=512, q_lora_rank=1536) [h=64, kv=64, hd=112]",262144,50000.0,7168.0,385.0,384.0,dense prefix from first_k_dense_replace; moe layers inferred from moe_layer_freq (+dense prefix if any); shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.n_routed_experts"", ""shared_experts_key"": ""primary.n_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""router_temp_key"": ""temperature""}"
nex-agi_deepseek-v3.1-nex-n1_free,1,61,0.0491803278688525,7168,1,0.142857142857143,9.0,48.5,2.57142857142857,0.285714285714286,2.57142857142857,73.4285714285714,0.11,1.0,526336.0,0.03125,18432.0,18432.0,2048.0,9.0,128,MLA,128,3,58.0,56,1024.0,0.0714285714285714,512.0,9218.9,derived head_dim from hidden_size/num_attention_heads,"MLA: kv_size_proxy uses kv_lora_rank (latent KV cache); MLA dims: kv_lora_rank=512, qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128","MLA (qk_rope_head_dim=64, qk_nope_head_dim=128, v_head_dim=128, kv_lora_rank=512, q_lora_rank=1536) [h=128, kv=128, hd=56]",131072,10000.0,7168.0,257.0,256.0,dense prefix from first_k_dense_replace; moe layers inferred from moe_layer_freq (+dense prefix if any); shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.n_routed_experts"", ""shared_experts_key"": ""primary.n_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
qwen_qwen3-235b-a22b,1,94,0,4096,2,0.25,2.3,40.4,,0.375,3,48,0.00,0.0,196608.0,0.0625,12288.0,,1536.0,8.0,64,GQA,4,0,94.0,128,1024.0,0.0625,4.0,2377.7,,,"GQA [h=64, kv=4, hd=128]",40960,1000000.0,8192.0,128.0,128.0,dense layers from mlp_only_layers,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.num_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""aux_loss_key"": ""router_aux_loss_coef""}"
qwen_qwen3-235b-a22b-thinking-2507,1,94,0,4096,2,0.25,2.3,40.4,,0.375,3,48,0.00,0.0,196608.0,0.0625,12288.0,,1536.0,8.0,64,GQA,4,0,94.0,128,1024.0,0.0625,4.0,2377.7,,,"GQA [h=64, kv=4, hd=128]",262144,5000000.0,8192.0,128.0,128.0,dense layers from mlp_only_layers,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.num_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""aux_loss_key"": ""router_aux_loss_coef""}"
qwen_qwen3-coder-30b-a3b-instruct,1,48,0,2048,2,0.5,2.3,40.4,,0.375,3,48,0.00,0.0,98304.0,0.0625,6144.0,,768.0,8.0,32,GQA,4,0,48.0,128,1024.0,0.125,4.0,2377.7,,,"GQA [h=32, kv=4, hd=128]",262144,10000000.0,4096.0,128.0,128.0,dense layers from mlp_only_layers,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.num_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""aux_loss_key"": ""router_aux_loss_coef""}"
qwen_qwen3-vl-235b-a22b-instruct,1,94,0,4096,2,0.25,2.3,40.4,,0.375,3,48,0.00,0.0,196608.0,0.0625,12288.0,,1536.0,8.0,64,GQA,4,0,94.0,128,1024.0,0.0625,4.0,2377.7,,,"GQA [h=64, kv=4, hd=128]",262144,5000000.0,8192.0,128.0,128.0,dense layers from mlp_only_layers,"{""primary_config"": ""text_config"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.num_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
qwen_qwen3-vl-235b-a22b-thinking,1,94,0,4096,2,0.25,2.3,40.4,,0.375,3,48,0.00,0.0,196608.0,0.0625,12288.0,,1536.0,8.0,64,GQA,4,0,94.0,128,1024.0,0.0625,4.0,2377.7,,,"GQA [h=64, kv=4, hd=128]",262144,5000000.0,8192.0,128.0,128.0,dense layers from mlp_only_layers,"{""primary_config"": ""text_config"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.num_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
stepfun-ai_step3,1,61,0,7168,2.28571428571429,4.57142857142857,6.0,14.1,,0.714285714285714,2.14285714285714,34.2857142857143,0.00,0.0,245760.0,0.0625,15360.0,,5120.0,3.0,64,Other,64,0,61.0,256,32768.0,1.0,64.0,197340.9,num_key_value_heads missing; assumed equal to num_attention_heads,,"MHA [h=64, kv=64, hd=256]",65536,500000.0,16384.0,48.0,48.0,moe layer indices unknown; assumed all layers are MoE,"{""primary_config"": ""text_config"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.moe_num_experts"", ""active_experts_key"": ""primary.moe_top_k"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""moe_shared_width_key"": ""primary.share_expert_dim"", ""attn_heads_key"": ""num_attention_heads"", ""attn_head_dim_key"": ""head_dim"", ""max_position_key"": ""max_seq_len"", ""rope_theta_key"": ""rope_theta""}"
tencent_hunyuan-a13b-instruct,1,32,0,4096,1,0.5,10.0,32.0,,0.75,6,48,0.00,0.0,196608.0,0.125,24576.0,,3072.0,8.0,32,MLA,8,0,32.0,128,2048.0,0.25,1024.0,20482.9,,MLA detected but kv_lora_rank missing; using kv_heads*head_dim as latent-rank proxy,"MLA [h=32, kv=8, hd=128]",32768,10000.0,4096.0,64.0,64.0,dense prefix from moe_layer_num_skipped; moe layer indices unknown; assumed all layers are MoE; active experts per token (routed) varies by layer from primary.moe_topk; averaged over MoE layers; routed expert width varies by layer from primary.moe_intermediate_size; averaged over MoE layers,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.num_experts"", ""active_experts_key"": ""primary.moe_topk"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""qk_norm_key"": ""use_qk_norm"", ""routing_topk_key"": ""moe_topk""}"
z-ai_glm-4.5,1,92,0.0326086956521739,5120,2.4,0.4,3.2,43.0,2.4,0.3,2.7,48.3,0.11,1.0,247296.0,0.05,13824.0,12288.0,1536.0,9.0,96,GQA,8,3,89.0,128,2048.0,0.0833333333333333,8.0,6492.0,,,"GQA [h=96, kv=8, hd=128]",131072,1000000.0,12288.0,161.0,160.0,dense prefix from first_k_dense_replace; moe layers inferred as all after dense prefix; shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.n_routed_experts"", ""shared_experts_key"": ""primary.n_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""partial_rope_key"": ""partial_rotary_factor"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""qk_norm_key"": ""use_qk_norm""}"
z-ai_glm-4.5-air,1,46,0.0217391304347826,4096,3,0.5,3.2,40.4,2.671875,0.34375,3.09375,44.34375,0.11,1.0,181632.0,0.0625,12672.0,10944.0,1408.0,9.0,96,GQA,8,1,45.0,128,2048.0,0.0833333333333333,8.0,6492.0,,,"GQA [h=96, kv=8, hd=128]",131072,1000000.0,12288.0,129.0,128.0,dense prefix from first_k_dense_replace; moe layers inferred as all after dense prefix; shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.n_routed_experts"", ""shared_experts_key"": ""primary.n_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""partial_rope_key"": ""partial_rotary_factor"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""qk_norm_key"": ""use_qk_norm""}"
z-ai_glm-4.5v,1,46,0.0217391304347826,4096,3,0.5,3.2,40.4,2.671875,0.34375,3.09375,44.34375,0.11,1.0,181632.0,0.0625,12672.0,10944.0,1408.0,9.0,96,GQA,8,1,45.0,128,2048.0,0.0833333333333333,8.0,6492.0,,,"GQA [h=96, kv=8, hd=128]",65536,10000.0,12288.0,129.0,128.0,dense prefix from first_k_dense_replace; moe layers inferred as all after dense prefix; shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""text_config"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.n_routed_experts"", ""shared_experts_key"": ""primary.n_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""partial_rope_key"": ""partial_rotary_factor"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""qk_norm_key"": ""use_qk_norm""}"
z-ai_glm-4.6,1,92,0.0326086956521739,5120,2.4,0.4,3.2,43.0,2.4,0.3,2.7,48.3,0.11,1.0,247296.0,0.05,13824.0,12288.0,1536.0,9.0,96,GQA,8,3,89.0,128,2048.0,0.0833333333333333,8.0,6492.0,,,"GQA [h=96, kv=8, hd=128]",202752,1000000.0,12288.0,161.0,160.0,dense prefix from first_k_dense_replace; moe layers inferred as all after dense prefix; shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.n_routed_experts"", ""shared_experts_key"": ""primary.n_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""partial_rope_key"": ""partial_rotary_factor"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""qk_norm_key"": ""use_qk_norm""}"
z-ai_glm-4.6v,1,46,0.0217391304347826,4096,3,0.5,3.2,40.4,2.671875,0.34375,3.09375,44.34375,0.11,1.0,181632.0,0.0625,12672.0,10944.0,1408.0,9.0,96,GQA,8,1,45.0,128,2048.0,0.0833333333333333,8.0,6492.0,,,"GQA [h=96, kv=8, hd=128]",131072,,12288.0,129.0,128.0,dense prefix from first_k_dense_replace; moe layers inferred as all after dense prefix; shared expert width missing; assumed equal to routed expert width; assumed per-token active expert count excludes shared; added shared experts separately in totals,"{""primary_config"": ""text_config"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""routed_experts_key"": ""primary.n_routed_experts"", ""shared_experts_key"": ""primary.n_shared_experts"", ""active_experts_key"": ""primary.num_experts_per_tok"", ""moe_routed_width_key"": ""primary.moe_intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""partial_rope_key"": ""partial_rotary_factor"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""qk_norm_key"": ""use_qk_norm""}"
CohereLabs_c4ai-command-a-03-2025,0,64,1,12288,1,0.166666666666667,3.2,,3,,,,,,,,,36864.0,,,96,GQA,8,0,0.0,128,2048.0,0.0833333333333333,8.0,6492.0,,,"GQA [h=96, kv=8, hd=128, sliding_window=4096]",131072,50000.0,12288.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""attn_sliding_window_key"": ""sliding_window"", ""norm_eps_key"": ""layer_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""partial_rope_key"": ""rotary_pct"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act""}"
Qwen_Qwen2.5-72B-Instruct,0,80,1,8192,1,0.25,3.2,,3.609375,,,,,,,,,29568.0,,,64,GQA,8,0,0.0,128,2048.0,0.125,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=64, kv=8, hd=128, sliding_window=131072]",32768,1000000.0,8192.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_sliding_window_key"": ""sliding_window"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
Qwen_Qwen2.5-Coder-32B-Instruct,0,64,1,5120,1,0.4,3.2,,5.4,,,,,,,,,27648.0,,,40,GQA,8,0,0.0,128,2048.0,0.2,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=40, kv=8, hd=128, sliding_window=131072]",32768,1000000.0,5120.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_sliding_window_key"": ""sliding_window"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
Sao10K_L3.1-70B-Euryale-v2.2,0,80,1,8192,1,0.25,3.2,,3.5,,,,,,,,,28672.0,,,64,GQA,8,0,0.0,128,2048.0,0.125,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=64, kv=8, hd=128]",131072,500000.0,8192.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
Sao10K_L3.3-70B-Euryale-v2.3,0,80,1,8192,1,0.25,3.2,,3.5,,,,,,,,,28672.0,,,64,GQA,8,0,0.0,128,2048.0,0.125,8.0,6492.0,,,"GQA [h=64, kv=8, hd=128]",131072,500000.0,8192.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
allenai_olmo-2-0325-32b-instruct,0,64,1,5120,1,0.4,3.2,,5.4,,,,,,,,,27648.0,,,40,GQA,8,0,0.0,128,2048.0,0.2,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=40, kv=8, hd=128]",4096,500000.0,5120.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
allenai_olmo-3-32b-think_free,0,64,1,5120,1,0.4,3.2,,5.4,,,,,,,,,27648.0,,,40,GQA,8,0,0.0,128,2048.0,0.2,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=40, kv=8, hd=128, sliding_window=4096]",65536,500000.0,5120.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_sliding_window_key"": ""sliding_window"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
alpindale_goliath-120b,0,137,1,8192,1,0.25,3.2,,3.5,,,,,,,,,28672.0,,,64,GQA,8,0,0.0,128,2048.0,0.125,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=64, kv=8, hd=128]",4096,10000.0,8192.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
anthracite-org_magnum-v4-72b,0,80,1,8192,1,0.25,3.2,,3.609375,,,,,,,,,29568.0,,,64,GQA,8,0,0.0,128,2048.0,0.125,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=64, kv=8, hd=128]",32768,1000000.0,8192.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
arliai_qwq-32b-arliai-rpr-v1,0,64,1,5120,1,0.4,3.2,,5.4,,,,,,,,,27648.0,,,40,GQA,8,0,0.0,128,2048.0,0.2,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=40, kv=8, hd=128]",40960,1000000.0,5120.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
deepcogito_cogito-v2-preview-llama-405b,0,126,1,16384,1,0.125,3.2,,3.25,,,,,,,,,53248.0,,,128,GQA,8,0,0.0,128,2048.0,0.0625,8.0,6492.0,,,"GQA [h=128, kv=8, hd=128]",131072,500000.0,16384.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
deepcogito_cogito-v2-preview-llama-70b,0,80,1,8192,1,0.25,3.2,,3.5,,,,,,,,,28672.0,,,64,GQA,8,0,0.0,128,2048.0,0.125,8.0,6492.0,,,"GQA [h=64, kv=8, hd=128]",131072,500000.0,8192.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
deepseek_deepseek-r1-distill-llama-70b,0,80,1,8192,1,0.25,3.2,,3.5,,,,,,,,,28672.0,,,64,GQA,8,0,0.0,128,2048.0,0.125,8.0,6492.0,,,"GQA [h=64, kv=8, hd=128]",131072,500000.0,8192.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
deepseek_deepseek-r1-distill-qwen-32b,0,64,1,5120,1,0.4,3.2,,5.4,,,,,,,,,27648.0,,,40,GQA,8,0,0.0,128,2048.0,0.2,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=40, kv=8, hd=128, sliding_window=131072]",131072,1000000.0,5120.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_sliding_window_key"": ""sliding_window"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
meta-llama_llama-3.1-405b,0,126,1,16384,1,0.125,3.2,,3.25,,,,,,,,,53248.0,,,128,GQA,8,0,0.0,128,2048.0,0.0625,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=128, kv=8, hd=128]",131072,500000.0,16384.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
meta-llama_llama-3.1-405b-instruct,0,126,1,16384,1,0.125,3.2,,3.25,,,,,,,,,53248.0,,,128,GQA,8,0,0.0,128,2048.0,0.0625,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=128, kv=8, hd=128]",131072,500000.0,16384.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
meta-llama_llama-3.1-70b-instruct,0,80,1,8192,1,0.25,3.2,,3.5,,,,,,,,,28672.0,,,64,GQA,8,0,0.0,128,2048.0,0.125,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=64, kv=8, hd=128]",131072,500000.0,8192.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
meta-llama_llama-3.3-70b-instruct,0,80,1,8192,1,0.25,3.2,,3.5,,,,,,,,,28672.0,,,64,GQA,8,0,0.0,128,2048.0,0.125,8.0,6492.0,,,"GQA [h=64, kv=8, hd=128]",131072,500000.0,8192.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
mistralai:Devstral-2-123B-Instruct-2512,0,88,1,12288,1,0.166666666666667,3.2,,2.33333333333333,,,,,,,,,28672.0,,,96,GQA,8,0,0.0,128,2048.0,0.0833333333333333,8.0,6492.0,,,"GQA [h=96, kv=8, hd=128]",262144,,12288.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
mistralai_Mistral-Large-Instruct-2411,0,88,1,12288,1,0.166666666666667,3.2,,2.33333333333333,,,,,,,,,28672.0,,,96,GQA,8,0,0.0,128,2048.0,0.0833333333333333,8.0,6492.0,,,"GQA [h=96, kv=8, hd=128]",131072,1000000.0,12288.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
mistralai_Pixtral-Large-Instruct-2411,0,88,1,12288,1,0.166666666666667,3.2,,2.33333333333333,,,,,,,,,28672.0,,,96,GQA,8,0,0.0,128,2048.0,0.0833333333333333,8.0,6492.0,,,"GQA [h=96, kv=8, hd=128]",131072,1000000000.0,12288.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.n_layers"", ""hidden_key"": ""primary.dim"", ""dense_ffn_key"": ""primary.hidden_dim"", ""attn_heads_key"": ""n_heads"", ""attn_kv_heads_key"": ""n_kv_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta""}"
moonshotai_kimi-dev-72b,0,80,1,8192,1,0.25,3.2,,3.609375,,,,,,,,,29568.0,,,64,GQA,8,0,0.0,128,2048.0,0.125,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=64, kv=8, hd=128, sliding_window=131072]",131072,1000000.0,8192.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_sliding_window_key"": ""sliding_window"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
nousresearch_hermes-3-llama-3.1-405b,0,126,1,16384,1,0.125,3.2,,3.25,,,,,,,,,53248.0,,,128,GQA,8,0,0.0,128,2048.0,0.0625,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=128, kv=8, hd=128]",131072,500000.0,16384.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
nousresearch_hermes-3-llama-3.1-405b_free,0,126,1,16384,1,0.125,3.2,,3.25,,,,,,,,,53248.0,,,128,GQA,8,0,0.0,128,2048.0,0.0625,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=128, kv=8, hd=128]",131072,500000.0,16384.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
nousresearch_hermes-4-70b,0,80,1,8192,1,0.25,3.2,,3.5,,,,,,,,,28672.0,,,64,GQA,8,0,0.0,128,2048.0,0.125,8.0,6492.0,,,"GQA [h=64, kv=8, hd=128]",131072,500000.0,8192.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
nvidia_Llama-3.1-Nemotron-70B-Instruct-HF,0,80,1,8192,1,0.25,3.2,,3.5,,,,,,,,,28672.0,,,64,GQA,8,0,0.0,128,2048.0,0.125,8.0,6492.0,,,"GQA [h=64, kv=8, hd=128]",131072,500000.0,8192.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
nvidia_Llama-3_1-Nemotron-Ultra-253B-v1,0,162,1,16384,1,2,7.0,,,,,,,,,,,,,,128,Other,128,0,0.0,128,32768.0,1.0,128.0,229743.9,num_key_value_heads missing; assumed equal to num_attention_heads; derived head_dim from hidden_size/num_attention_heads,,"MHA [h=128, kv=128, hd=128]",131072,500000.0,16384.0,,,"Perplexity: The Hugging Face config for nvidia/Llama-3_1-Nemotron-Ultra-253B-v1 sets ""hidden_size"": 16384, ""num_hidden_layers"": 162, and ""intermediate_size"": null, and instead provides a per-layer ""block_configs"" array where each block has an FFN config with ""ffn_mult"" and flags such as ""no_op"" and ""replace_with_linear"". [huggingface.co](https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1/blob/main/config.json) The FFN configuration code (FFNConfig) in block_config.py enforces that when an FFN is active (no_op = False and replace_with_linear = False), ffn_mult must be non-null and is rounded to 6 decimal places; when FFN is disabled or replaced with a linear layer, ffn_mult is forced to None. [huggingface.co](https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1/blob/main/block_config.py) However, neither the public config nor the helper classes expose the exact formula that DeciLMForCausalLM uses internally to map hidden_size and ffn_mult to a concrete dense FFN intermediate dimension (d_ff), and the model card and NVIDIA NeMo Llama Nemotron documentation describe FFN Fusion and per-block expansion/compression ratios qualitatively without specifying a closed-form size formula. [huggingface.co](https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1) [docs.nvidia.com](https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/llama_nemotron.html) Because the implementation of modeling_decilm.DeciLMForCausalLM (referenced via auto_map in the config) is not included in the accessible files, it is not possible from the available public artifacts alone to reconstruct reliable per-layer intermediate sizes or an average d_ff; any attempt to do so would require reverse-engineering proprietary code or making unverified assumptions about how ffn_mult scales and how FFN Fusion reshapes the layers.; Perplexity: intermediate_size is null and per-layer FFN sizes can't be reconstructed from public artifacts; leaving dense_ffn_intermediate_size blank.","{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""external_perplexity_override"": true, ""attn_heads_key"": ""num_attention_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
opengvlab_internvl3-78b,0,80,1,8192,1,0.25,3.2,,3.609375,,,,,,,,,29568.0,,,64,GQA,8,0,0.0,128,2048.0,0.125,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=64, kv=8, hd=128]",32768,1000000.0,8192.0,,,User confirmed this model is dense (not MoE); MoE columns intentionally blank,"{""primary_config"": ""llm_config"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings"", ""router_temp_key"": ""temperature""}"
qwen_qwen2.5-vl-32b-instruct,0,64,1,5120,1,0.4,3.2,,5.4,,,,,,,,,27648.0,,,40,GQA,8,0,0.0,128,2048.0,0.2,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=40, kv=8, hd=128, sliding_window=32768]",128000,1000000.0,5120.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_sliding_window_key"": ""sliding_window"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
qwen_qwen2.5-vl-72b-instruct,0,80,1,8192,1,0.25,3.2,,3.609375,,,,,,,,,29568.0,,,64,GQA,8,0,0.0,128,2048.0,0.125,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=64, kv=8, hd=128, sliding_window=32768]",128000,1000000.0,8192.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_sliding_window_key"": ""sliding_window"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
qwen_qwen3-32b,0,64,1,5120,1.6,0.4,3.2,,5,,,,,,,,,25600.0,,,64,GQA,8,0,0.0,128,2048.0,0.125,8.0,6492.0,,,"GQA [h=64, kv=8, hd=128]",40960,1000000.0,8192.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
qwen_qwq-32b,0,64,1,5120,1,0.4,3.2,,5.4,,,,,,,,,27648.0,,,40,GQA,8,0,0.0,128,2048.0,0.2,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=40, kv=8, hd=128, sliding_window=32768]",40960,1000000.0,5120.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_sliding_window_key"": ""sliding_window"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
sao10k_l3.1-70b-hanami-x1,0,80,1,8192,1,0.25,3.2,,3.5,,,,,,,,,28672.0,,,64,GQA,8,0,0.0,128,2048.0,0.125,8.0,6492.0,derived head_dim from hidden_size/num_attention_heads,,"GQA [h=64, kv=8, hd=128]",131072,500000.0,8192.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
z-ai_glm-4-32b,0,61,1,6144,1,0.0833333333333333,1.6,,3.75,,,,,,,,,23040.0,,,48,GQA,2,0,0.0,128,512.0,0.0416666666666666,2.0,811.5,,,"GQA [h=48, kv=2, hd=128]",32768,10000.0,6144.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""attn_head_dim_key"": ""head_dim"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""partial_rope_key"": ""partial_rotary_factor"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
meta-llama/Meta-Llama-3-70B-Instruct,0,80,1,8192,1,0.25,3.2,,3.5,,,,,,,,,28672.0,,,64,GQA,8,0,0.0,128,2048.0,0.125,8.0,6492.0,,,"GQA [h=64, kv=8, hd=128]",8192,500000.0,8192.0,,,,"{""primary_config"": ""root"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""attention_dropout_key"": ""attention_dropout"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"
meta-llama/Llama-3.2-90B-Vision-Instruct,0,100,1,8192,1,0.25,3.2,,3.5,,,,,,,,,28672.0,,,64,GQA,8,0,0.0,128,2048.0,0.125,8.0,6492.0,,,"GQA [h=64, kv=8, hd=128]",131072,500000.0,8192.0,,,,"{""primary_config"": ""text_config"", ""num_layers_key"": ""primary.num_hidden_layers"", ""hidden_key"": ""primary.hidden_size"", ""dense_ffn_key"": ""primary.intermediate_size"", ""attn_heads_key"": ""num_attention_heads"", ""attn_kv_heads_key"": ""num_key_value_heads"", ""norm_eps_key"": ""rms_norm_eps"", ""max_position_key"": ""max_position_embeddings"", ""rope_theta_key"": ""rope_theta"", ""rope_scaling_key"": ""rope_scaling"", ""mlp_activation_key"": ""hidden_act"", ""tie_embeddings_key"": ""tie_word_embeddings""}"